{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e029d033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "#\n",
    "# MVP: create a version that allows us to pass and no more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded5e586",
   "metadata": {},
   "source": [
    "Copied from assignment 2. NEEDS WORK!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4547e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize data for pytorch DataLoader\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "DATA_DIR = '/Users/hartih/Documents/School/Deep learning/Final_project/dl2021-image-corpus-proj/'\n",
    "ANNOTATIONS_DIR = DATA_DIR + 'annotations/'\n",
    "IMAGES_DIR = DATA_DIR + 'images/'\n",
    "\n",
    "# New fodlers for train, test, and dev sets\n",
    "TRAIN_DIR = DATA_DIR + 'train/'\n",
    "DEV_DIR = DATA_DIR + 'dev/'\n",
    "TEST_DIR = DATA_DIR + 'test/'\n",
    "\n",
    "annotations = [\"baby\",\n",
    "               \"bird\",\n",
    "               \"car\",\n",
    "               \"clouds\",\n",
    "               \"dog\",\n",
    "               \"female\",\n",
    "               \"flower\",\n",
    "               \"male\",\n",
    "               \"night\",\n",
    "               \"people\",\n",
    "               \"portrait\",\n",
    "               \"river\",\n",
    "               \"sea\",\n",
    "               \"tree\"]\n",
    "\n",
    "# Create \"train\" folder for data\n",
    "os.makedirs(TRAIN_DIR)\n",
    "\n",
    "for annotation in annotations:\n",
    "    \n",
    "    # Create folder for this annotation within \"train\" folder\n",
    "    os.makedirs(TRAIN_DIR + annotation)\n",
    "    os.makedirs(DEV_DIR + annotation)\n",
    "    os.makedirs(TEST_DIR + annotation)\n",
    "\n",
    "    annotation_file = open(ANNOTATIONS_DIR + annotation + \".txt\", \"r\")\n",
    "    annotation_data = annotation_file.read()\n",
    "    annotation_file.close()\n",
    "    data_into_list = annotation_data.split('\\n')\n",
    "    image_file_names = [\"im\" + i + \".jpg\" for i in data_into_list]\n",
    "\n",
    "    for image_file_name in image_file_names:\n",
    "        if os.path.isfile(IMAGES_DIR + image_file_name):\n",
    "            division = random.randint(1, 3)\n",
    "            if division == 1:\n",
    "                shutil.copyfile(IMAGES_DIR + image_file_name, TRAIN_DIR + annotation + \"/\" + image_file_name)\n",
    "            if division == 2:\n",
    "                shutil.copyfile(IMAGES_DIR + image_file_name, DEV_DIR + annotation + \"/\" + image_file_name)\n",
    "            if division == 3:\n",
    "                shutil.copyfile(IMAGES_DIR + image_file_name, TEST_DIR + annotation + \"/\" + image_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ccd4ed4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hartih/opt/anaconda3/envs/deep_learning/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch 1 - Batch 1/68: Loss: 2.6461 | Train Acc: 1.000% (1/100)\n",
      "Training: Epoch 1 - Batch 2/68: Loss: 2.5151 | Train Acc: 10.000% (20/200)\n",
      "Training: Epoch 1 - Batch 3/68: Loss: 2.3695 | Train Acc: 17.000% (51/300)\n",
      "Training: Epoch 1 - Batch 4/68: Loss: 2.3722 | Train Acc: 18.000% (72/400)\n",
      "Training: Epoch 1 - Batch 5/68: Loss: 2.3395 | Train Acc: 17.200% (86/500)\n",
      "Training: Epoch 1 - Batch 6/68: Loss: 2.2913 | Train Acc: 21.000% (126/600)\n",
      "Training: Epoch 1 - Batch 7/68: Loss: 2.2828 | Train Acc: 22.000% (154/700)\n",
      "Training: Epoch 1 - Batch 8/68: Loss: 2.2884 | Train Acc: 22.625% (181/800)\n",
      "Training: Epoch 1 - Batch 9/68: Loss: 2.2862 | Train Acc: 24.111% (217/900)\n",
      "Training: Epoch 1 - Batch 10/68: Loss: 2.2763 | Train Acc: 24.700% (247/1000)\n",
      "Training: Epoch 1 - Batch 11/68: Loss: 2.2650 | Train Acc: 25.545% (281/1100)\n",
      "Training: Epoch 1 - Batch 12/68: Loss: 2.2537 | Train Acc: 26.083% (313/1200)\n",
      "Training: Epoch 1 - Batch 13/68: Loss: 2.2473 | Train Acc: 26.000% (338/1300)\n",
      "Training: Epoch 1 - Batch 14/68: Loss: 2.2432 | Train Acc: 26.214% (367/1400)\n",
      "Training: Epoch 1 - Batch 15/68: Loss: 2.2323 | Train Acc: 26.333% (395/1500)\n",
      "Training: Epoch 1 - Batch 16/68: Loss: 2.2256 | Train Acc: 26.688% (427/1600)\n",
      "Training: Epoch 1 - Batch 17/68: Loss: 2.2108 | Train Acc: 27.118% (461/1700)\n",
      "Training: Epoch 1 - Batch 18/68: Loss: 2.1981 | Train Acc: 26.944% (485/1800)\n",
      "Training: Epoch 1 - Batch 19/68: Loss: 2.1911 | Train Acc: 26.947% (512/1900)\n",
      "Training: Epoch 1 - Batch 20/68: Loss: 2.1817 | Train Acc: 27.550% (551/2000)\n",
      "Training: Epoch 1 - Batch 21/68: Loss: 2.1829 | Train Acc: 27.667% (581/2100)\n",
      "Training: Epoch 1 - Batch 22/68: Loss: 2.1741 | Train Acc: 28.136% (619/2200)\n",
      "Training: Epoch 1 - Batch 23/68: Loss: 2.1741 | Train Acc: 28.217% (649/2300)\n",
      "Training: Epoch 1 - Batch 24/68: Loss: 2.1689 | Train Acc: 28.083% (674/2400)\n",
      "Training: Epoch 1 - Batch 25/68: Loss: 2.1555 | Train Acc: 28.520% (713/2500)\n",
      "Training: Epoch 1 - Batch 26/68: Loss: 2.1507 | Train Acc: 28.692% (746/2600)\n",
      "Training: Epoch 1 - Batch 27/68: Loss: 2.1470 | Train Acc: 28.815% (778/2700)\n",
      "Training: Epoch 1 - Batch 28/68: Loss: 2.1400 | Train Acc: 28.821% (807/2800)\n",
      "Training: Epoch 1 - Batch 29/68: Loss: 2.1362 | Train Acc: 29.000% (841/2900)\n",
      "Training: Epoch 1 - Batch 30/68: Loss: 2.1310 | Train Acc: 29.333% (880/3000)\n",
      "Training: Epoch 1 - Batch 31/68: Loss: 2.1236 | Train Acc: 29.355% (910/3100)\n",
      "Training: Epoch 1 - Batch 32/68: Loss: 2.1192 | Train Acc: 29.375% (940/3200)\n",
      "Training: Epoch 1 - Batch 33/68: Loss: 2.1141 | Train Acc: 29.394% (970/3300)\n",
      "Training: Epoch 1 - Batch 34/68: Loss: 2.1086 | Train Acc: 29.294% (996/3400)\n",
      "Training: Epoch 1 - Batch 35/68: Loss: 2.1083 | Train Acc: 29.286% (1025/3500)\n",
      "Training: Epoch 1 - Batch 36/68: Loss: 2.1095 | Train Acc: 29.528% (1063/3600)\n",
      "Training: Epoch 1 - Batch 37/68: Loss: 2.1033 | Train Acc: 29.865% (1105/3700)\n",
      "Training: Epoch 1 - Batch 38/68: Loss: 2.0988 | Train Acc: 30.053% (1142/3800)\n",
      "Training: Epoch 1 - Batch 39/68: Loss: 2.0954 | Train Acc: 29.949% (1168/3900)\n",
      "Training: Epoch 1 - Batch 40/68: Loss: 2.0926 | Train Acc: 29.975% (1199/4000)\n",
      "Training: Epoch 1 - Batch 41/68: Loss: 2.0911 | Train Acc: 30.073% (1233/4100)\n",
      "Training: Epoch 1 - Batch 42/68: Loss: 2.0923 | Train Acc: 29.929% (1257/4200)\n",
      "Training: Epoch 1 - Batch 43/68: Loss: 2.0899 | Train Acc: 30.116% (1295/4300)\n",
      "Training: Epoch 1 - Batch 44/68: Loss: 2.0869 | Train Acc: 30.250% (1331/4400)\n",
      "Training: Epoch 1 - Batch 45/68: Loss: 2.0874 | Train Acc: 30.067% (1353/4500)\n",
      "Training: Epoch 1 - Batch 46/68: Loss: 2.0844 | Train Acc: 30.109% (1385/4600)\n",
      "Training: Epoch 1 - Batch 47/68: Loss: 2.0834 | Train Acc: 30.277% (1423/4700)\n",
      "Training: Epoch 1 - Batch 48/68: Loss: 2.0819 | Train Acc: 30.312% (1455/4800)\n",
      "Training: Epoch 1 - Batch 49/68: Loss: 2.0776 | Train Acc: 30.469% (1493/4900)\n",
      "Training: Epoch 1 - Batch 50/68: Loss: 2.0759 | Train Acc: 30.340% (1517/5000)\n",
      "Training: Epoch 1 - Batch 51/68: Loss: 2.0702 | Train Acc: 30.392% (1550/5100)\n",
      "Training: Epoch 1 - Batch 52/68: Loss: 2.0673 | Train Acc: 30.404% (1581/5200)\n",
      "Training: Epoch 1 - Batch 53/68: Loss: 2.0715 | Train Acc: 30.340% (1608/5300)\n",
      "Training: Epoch 1 - Batch 54/68: Loss: 2.0718 | Train Acc: 30.241% (1633/5400)\n",
      "Training: Epoch 1 - Batch 55/68: Loss: 2.0722 | Train Acc: 30.145% (1658/5500)\n",
      "Training: Epoch 1 - Batch 56/68: Loss: 2.0694 | Train Acc: 30.107% (1686/5600)\n",
      "Training: Epoch 1 - Batch 57/68: Loss: 2.0654 | Train Acc: 30.193% (1721/5700)\n",
      "Training: Epoch 1 - Batch 58/68: Loss: 2.0644 | Train Acc: 30.155% (1749/5800)\n",
      "Training: Epoch 1 - Batch 59/68: Loss: 2.0641 | Train Acc: 30.237% (1784/5900)\n",
      "Training: Epoch 1 - Batch 60/68: Loss: 2.0613 | Train Acc: 30.300% (1818/6000)\n",
      "Training: Epoch 1 - Batch 61/68: Loss: 2.0621 | Train Acc: 30.311% (1849/6100)\n",
      "Training: Epoch 1 - Batch 62/68: Loss: 2.0590 | Train Acc: 30.355% (1882/6200)\n",
      "Training: Epoch 1 - Batch 63/68: Loss: 2.0582 | Train Acc: 30.429% (1917/6300)\n",
      "Training: Epoch 1 - Batch 64/68: Loss: 2.0555 | Train Acc: 30.438% (1948/6400)\n",
      "Training: Epoch 1 - Batch 65/68: Loss: 2.0552 | Train Acc: 30.400% (1976/6500)\n",
      "Training: Epoch 1 - Batch 66/68: Loss: 2.0529 | Train Acc: 30.348% (2003/6600)\n",
      "Training: Epoch 1 - Batch 67/68: Loss: 2.0520 | Train Acc: 30.373% (2035/6700)\n",
      "Training: Epoch 1 - Batch 68/68: Loss: 2.0498 | Train Acc: 30.394% (2054/6758)\n",
      "Evaluating: Batch 6645/6645: Loss: 1.9582 | Dev Acc: 28.969% (1925/6645)\n",
      "Training: Epoch 2 - Batch 1/68: Loss: 2.2674 | Train Acc: 23.000% (23/100)\n",
      "Training: Epoch 2 - Batch 2/68: Loss: 2.0901 | Train Acc: 23.500% (47/200)\n",
      "Training: Epoch 2 - Batch 3/68: Loss: 2.0726 | Train Acc: 23.333% (70/300)\n",
      "Training: Epoch 2 - Batch 4/68: Loss: 2.0325 | Train Acc: 25.750% (103/400)\n",
      "Training: Epoch 2 - Batch 5/68: Loss: 2.0078 | Train Acc: 27.600% (138/500)\n",
      "Training: Epoch 2 - Batch 6/68: Loss: 1.9868 | Train Acc: 28.333% (170/600)\n",
      "Training: Epoch 2 - Batch 7/68: Loss: 1.9801 | Train Acc: 28.857% (202/700)\n",
      "Training: Epoch 2 - Batch 8/68: Loss: 1.9745 | Train Acc: 28.750% (230/800)\n",
      "Training: Epoch 2 - Batch 9/68: Loss: 1.9765 | Train Acc: 29.222% (263/900)\n",
      "Training: Epoch 2 - Batch 10/68: Loss: 1.9851 | Train Acc: 28.400% (284/1000)\n",
      "Training: Epoch 2 - Batch 11/68: Loss: 1.9898 | Train Acc: 28.182% (310/1100)\n",
      "Training: Epoch 2 - Batch 12/68: Loss: 1.9847 | Train Acc: 28.333% (340/1200)\n",
      "Training: Epoch 2 - Batch 13/68: Loss: 1.9860 | Train Acc: 28.462% (370/1300)\n",
      "Training: Epoch 2 - Batch 14/68: Loss: 1.9732 | Train Acc: 28.714% (402/1400)\n",
      "Training: Epoch 2 - Batch 15/68: Loss: 1.9724 | Train Acc: 28.933% (434/1500)\n",
      "Training: Epoch 2 - Batch 16/68: Loss: 1.9725 | Train Acc: 29.125% (466/1600)\n",
      "Training: Epoch 2 - Batch 17/68: Loss: 1.9753 | Train Acc: 28.941% (492/1700)\n",
      "Training: Epoch 2 - Batch 18/68: Loss: 1.9721 | Train Acc: 29.111% (524/1800)\n",
      "Training: Epoch 2 - Batch 19/68: Loss: 1.9747 | Train Acc: 29.105% (553/1900)\n",
      "Training: Epoch 2 - Batch 20/68: Loss: 1.9659 | Train Acc: 29.200% (584/2000)\n",
      "Training: Epoch 2 - Batch 21/68: Loss: 1.9676 | Train Acc: 29.238% (614/2100)\n",
      "Training: Epoch 2 - Batch 22/68: Loss: 1.9586 | Train Acc: 29.545% (650/2200)\n",
      "Training: Epoch 2 - Batch 23/68: Loss: 1.9552 | Train Acc: 29.478% (678/2300)\n",
      "Training: Epoch 2 - Batch 24/68: Loss: 1.9499 | Train Acc: 29.750% (714/2400)\n",
      "Training: Epoch 2 - Batch 25/68: Loss: 1.9453 | Train Acc: 30.120% (753/2500)\n",
      "Training: Epoch 2 - Batch 26/68: Loss: 1.9424 | Train Acc: 30.462% (792/2600)\n",
      "Training: Epoch 2 - Batch 27/68: Loss: 1.9489 | Train Acc: 30.185% (815/2700)\n",
      "Training: Epoch 2 - Batch 28/68: Loss: 1.9499 | Train Acc: 30.036% (841/2800)\n",
      "Training: Epoch 2 - Batch 29/68: Loss: 1.9474 | Train Acc: 30.172% (875/2900)\n",
      "Training: Epoch 2 - Batch 30/68: Loss: 1.9470 | Train Acc: 30.200% (906/3000)\n",
      "Training: Epoch 2 - Batch 31/68: Loss: 1.9422 | Train Acc: 30.194% (936/3100)\n",
      "Training: Epoch 2 - Batch 32/68: Loss: 1.9373 | Train Acc: 30.281% (969/3200)\n",
      "Training: Epoch 2 - Batch 33/68: Loss: 1.9379 | Train Acc: 30.121% (994/3300)\n",
      "Training: Epoch 2 - Batch 34/68: Loss: 1.9328 | Train Acc: 30.382% (1033/3400)\n",
      "Training: Epoch 2 - Batch 35/68: Loss: 1.9350 | Train Acc: 30.257% (1059/3500)\n",
      "Training: Epoch 2 - Batch 36/68: Loss: 1.9371 | Train Acc: 30.139% (1085/3600)\n",
      "Training: Epoch 2 - Batch 37/68: Loss: 1.9386 | Train Acc: 30.189% (1117/3700)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch 2 - Batch 38/68: Loss: 1.9369 | Train Acc: 30.368% (1154/3800)\n",
      "Training: Epoch 2 - Batch 39/68: Loss: 1.9324 | Train Acc: 30.667% (1196/3900)\n",
      "Training: Epoch 2 - Batch 40/68: Loss: 1.9326 | Train Acc: 30.700% (1228/4000)\n",
      "Training: Epoch 2 - Batch 41/68: Loss: 1.9270 | Train Acc: 30.902% (1267/4100)\n",
      "Training: Epoch 2 - Batch 42/68: Loss: 1.9258 | Train Acc: 30.929% (1299/4200)\n",
      "Training: Epoch 2 - Batch 43/68: Loss: 1.9268 | Train Acc: 30.837% (1326/4300)\n",
      "Training: Epoch 2 - Batch 44/68: Loss: 1.9229 | Train Acc: 31.045% (1366/4400)\n",
      "Training: Epoch 2 - Batch 45/68: Loss: 1.9222 | Train Acc: 31.133% (1401/4500)\n",
      "Training: Epoch 2 - Batch 46/68: Loss: 1.9221 | Train Acc: 31.065% (1429/4600)\n",
      "Training: Epoch 2 - Batch 47/68: Loss: 1.9241 | Train Acc: 31.000% (1457/4700)\n",
      "Training: Epoch 2 - Batch 48/68: Loss: 1.9232 | Train Acc: 30.917% (1484/4800)\n",
      "Training: Epoch 2 - Batch 49/68: Loss: 1.9234 | Train Acc: 31.000% (1519/4900)\n",
      "Training: Epoch 2 - Batch 50/68: Loss: 1.9221 | Train Acc: 31.100% (1555/5000)\n",
      "Training: Epoch 2 - Batch 51/68: Loss: 1.9211 | Train Acc: 31.020% (1582/5100)\n",
      "Training: Epoch 2 - Batch 52/68: Loss: 1.9232 | Train Acc: 30.981% (1611/5200)\n",
      "Training: Epoch 2 - Batch 53/68: Loss: 1.9223 | Train Acc: 31.038% (1645/5300)\n",
      "Training: Epoch 2 - Batch 54/68: Loss: 1.9202 | Train Acc: 31.037% (1676/5400)\n",
      "Training: Epoch 2 - Batch 55/68: Loss: 1.9188 | Train Acc: 31.000% (1705/5500)\n",
      "Training: Epoch 2 - Batch 56/68: Loss: 1.9207 | Train Acc: 31.000% (1736/5600)\n",
      "Training: Epoch 2 - Batch 57/68: Loss: 1.9244 | Train Acc: 30.825% (1757/5700)\n",
      "Training: Epoch 2 - Batch 58/68: Loss: 1.9232 | Train Acc: 30.897% (1792/5800)\n",
      "Training: Epoch 2 - Batch 59/68: Loss: 1.9227 | Train Acc: 30.966% (1827/5900)\n",
      "Training: Epoch 2 - Batch 60/68: Loss: 1.9198 | Train Acc: 31.050% (1863/6000)\n",
      "Training: Epoch 2 - Batch 61/68: Loss: 1.9177 | Train Acc: 31.164% (1901/6100)\n",
      "Training: Epoch 2 - Batch 62/68: Loss: 1.9188 | Train Acc: 31.177% (1933/6200)\n",
      "Training: Epoch 2 - Batch 63/68: Loss: 1.9176 | Train Acc: 31.111% (1960/6300)\n",
      "Training: Epoch 2 - Batch 64/68: Loss: 1.9179 | Train Acc: 31.109% (1991/6400)\n",
      "Training: Epoch 2 - Batch 65/68: Loss: 1.9194 | Train Acc: 31.062% (2019/6500)\n",
      "Training: Epoch 2 - Batch 66/68: Loss: 1.9199 | Train Acc: 31.015% (2047/6600)\n",
      "Training: Epoch 2 - Batch 67/68: Loss: 1.9189 | Train Acc: 31.090% (2083/6700)\n",
      "Training: Epoch 2 - Batch 68/68: Loss: 1.9165 | Train Acc: 31.163% (2106/6758)\n",
      "Evaluating: Batch 6645/6645: Loss: 1.9055 | Dev Acc: 31.211% (2074/6645)\n",
      "Training: Epoch 3 - Batch 1/68: Loss: 1.7423 | Train Acc: 32.000% (32/100)\n",
      "Training: Epoch 3 - Batch 2/68: Loss: 1.8205 | Train Acc: 29.500% (59/200)\n",
      "Training: Epoch 3 - Batch 3/68: Loss: 1.8073 | Train Acc: 34.333% (103/300)\n",
      "Training: Epoch 3 - Batch 4/68: Loss: 1.7983 | Train Acc: 34.750% (139/400)\n",
      "Training: Epoch 3 - Batch 5/68: Loss: 1.8555 | Train Acc: 32.800% (164/500)\n",
      "Training: Epoch 3 - Batch 6/68: Loss: 1.8455 | Train Acc: 34.167% (205/600)\n",
      "Training: Epoch 3 - Batch 7/68: Loss: 1.8629 | Train Acc: 32.857% (230/700)\n",
      "Training: Epoch 3 - Batch 8/68: Loss: 1.8462 | Train Acc: 33.500% (268/800)\n",
      "Training: Epoch 3 - Batch 9/68: Loss: 1.8354 | Train Acc: 33.778% (304/900)\n",
      "Training: Epoch 3 - Batch 10/68: Loss: 1.8315 | Train Acc: 34.800% (348/1000)\n",
      "Training: Epoch 3 - Batch 11/68: Loss: 1.8416 | Train Acc: 34.545% (380/1100)\n",
      "Training: Epoch 3 - Batch 12/68: Loss: 1.8349 | Train Acc: 34.917% (419/1200)\n",
      "Training: Epoch 3 - Batch 13/68: Loss: 1.8354 | Train Acc: 34.615% (450/1300)\n",
      "Training: Epoch 3 - Batch 14/68: Loss: 1.8298 | Train Acc: 34.929% (489/1400)\n",
      "Training: Epoch 3 - Batch 15/68: Loss: 1.8244 | Train Acc: 34.533% (518/1500)\n",
      "Training: Epoch 3 - Batch 16/68: Loss: 1.8268 | Train Acc: 33.938% (543/1600)\n",
      "Training: Epoch 3 - Batch 17/68: Loss: 1.8276 | Train Acc: 33.529% (570/1700)\n",
      "Training: Epoch 3 - Batch 18/68: Loss: 1.8317 | Train Acc: 33.444% (602/1800)\n",
      "Training: Epoch 3 - Batch 19/68: Loss: 1.8316 | Train Acc: 33.368% (634/1900)\n",
      "Training: Epoch 3 - Batch 20/68: Loss: 1.8393 | Train Acc: 33.150% (663/2000)\n",
      "Training: Epoch 3 - Batch 21/68: Loss: 1.8376 | Train Acc: 33.238% (698/2100)\n",
      "Training: Epoch 3 - Batch 22/68: Loss: 1.8391 | Train Acc: 33.000% (726/2200)\n",
      "Training: Epoch 3 - Batch 23/68: Loss: 1.8423 | Train Acc: 32.913% (757/2300)\n",
      "Training: Epoch 3 - Batch 24/68: Loss: 1.8435 | Train Acc: 32.500% (780/2400)\n",
      "Training: Epoch 3 - Batch 25/68: Loss: 1.8432 | Train Acc: 32.320% (808/2500)\n",
      "Training: Epoch 3 - Batch 26/68: Loss: 1.8403 | Train Acc: 32.269% (839/2600)\n",
      "Training: Epoch 3 - Batch 27/68: Loss: 1.8394 | Train Acc: 32.259% (871/2700)\n",
      "Training: Epoch 3 - Batch 28/68: Loss: 1.8311 | Train Acc: 32.536% (911/2800)\n",
      "Training: Epoch 3 - Batch 29/68: Loss: 1.8290 | Train Acc: 32.655% (947/2900)\n",
      "Training: Epoch 3 - Batch 30/68: Loss: 1.8270 | Train Acc: 32.800% (984/3000)\n",
      "Training: Epoch 3 - Batch 31/68: Loss: 1.8297 | Train Acc: 32.774% (1016/3100)\n",
      "Training: Epoch 3 - Batch 32/68: Loss: 1.8340 | Train Acc: 32.656% (1045/3200)\n",
      "Training: Epoch 3 - Batch 33/68: Loss: 1.8344 | Train Acc: 32.606% (1076/3300)\n",
      "Training: Epoch 3 - Batch 34/68: Loss: 1.8294 | Train Acc: 32.765% (1114/3400)\n",
      "Training: Epoch 3 - Batch 35/68: Loss: 1.8281 | Train Acc: 32.743% (1146/3500)\n",
      "Training: Epoch 3 - Batch 36/68: Loss: 1.8273 | Train Acc: 32.861% (1183/3600)\n",
      "Training: Epoch 3 - Batch 37/68: Loss: 1.8267 | Train Acc: 33.054% (1223/3700)\n",
      "Training: Epoch 3 - Batch 38/68: Loss: 1.8272 | Train Acc: 33.211% (1262/3800)\n",
      "Training: Epoch 3 - Batch 39/68: Loss: 1.8278 | Train Acc: 33.128% (1292/3900)\n",
      "Training: Epoch 3 - Batch 40/68: Loss: 1.8312 | Train Acc: 32.825% (1313/4000)\n",
      "Training: Epoch 3 - Batch 41/68: Loss: 1.8324 | Train Acc: 32.732% (1342/4100)\n",
      "Training: Epoch 3 - Batch 42/68: Loss: 1.8345 | Train Acc: 32.714% (1374/4200)\n",
      "Training: Epoch 3 - Batch 43/68: Loss: 1.8343 | Train Acc: 32.674% (1405/4300)\n",
      "Training: Epoch 3 - Batch 44/68: Loss: 1.8354 | Train Acc: 32.523% (1431/4400)\n",
      "Training: Epoch 3 - Batch 45/68: Loss: 1.8346 | Train Acc: 32.578% (1466/4500)\n",
      "Training: Epoch 3 - Batch 46/68: Loss: 1.8333 | Train Acc: 32.609% (1500/4600)\n",
      "Training: Epoch 3 - Batch 47/68: Loss: 1.8322 | Train Acc: 32.553% (1530/4700)\n",
      "Training: Epoch 3 - Batch 48/68: Loss: 1.8315 | Train Acc: 32.667% (1568/4800)\n",
      "Training: Epoch 3 - Batch 49/68: Loss: 1.8299 | Train Acc: 32.776% (1606/4900)\n",
      "Training: Epoch 3 - Batch 50/68: Loss: 1.8290 | Train Acc: 32.840% (1642/5000)\n",
      "Training: Epoch 3 - Batch 51/68: Loss: 1.8281 | Train Acc: 32.941% (1680/5100)\n",
      "Training: Epoch 3 - Batch 52/68: Loss: 1.8279 | Train Acc: 33.077% (1720/5200)\n",
      "Training: Epoch 3 - Batch 53/68: Loss: 1.8293 | Train Acc: 33.000% (1749/5300)\n",
      "Training: Epoch 3 - Batch 54/68: Loss: 1.8276 | Train Acc: 33.093% (1787/5400)\n",
      "Training: Epoch 3 - Batch 55/68: Loss: 1.8277 | Train Acc: 33.000% (1815/5500)\n",
      "Training: Epoch 3 - Batch 56/68: Loss: 1.8280 | Train Acc: 32.982% (1847/5600)\n",
      "Training: Epoch 3 - Batch 57/68: Loss: 1.8299 | Train Acc: 32.912% (1876/5700)\n",
      "Training: Epoch 3 - Batch 58/68: Loss: 1.8316 | Train Acc: 32.897% (1908/5800)\n",
      "Training: Epoch 3 - Batch 59/68: Loss: 1.8344 | Train Acc: 32.847% (1938/5900)\n",
      "Training: Epoch 3 - Batch 60/68: Loss: 1.8368 | Train Acc: 32.717% (1963/6000)\n",
      "Training: Epoch 3 - Batch 61/68: Loss: 1.8355 | Train Acc: 32.689% (1994/6100)\n",
      "Training: Epoch 3 - Batch 62/68: Loss: 1.8327 | Train Acc: 32.758% (2031/6200)\n",
      "Training: Epoch 3 - Batch 63/68: Loss: 1.8322 | Train Acc: 32.794% (2066/6300)\n",
      "Training: Epoch 3 - Batch 64/68: Loss: 1.8309 | Train Acc: 32.875% (2104/6400)\n",
      "Training: Epoch 3 - Batch 65/68: Loss: 1.8309 | Train Acc: 32.831% (2134/6500)\n",
      "Training: Epoch 3 - Batch 66/68: Loss: 1.8337 | Train Acc: 32.758% (2162/6600)\n",
      "Training: Epoch 3 - Batch 67/68: Loss: 1.8319 | Train Acc: 32.881% (2203/6700)\n",
      "Training: Epoch 3 - Batch 68/68: Loss: 1.8325 | Train Acc: 32.894% (2223/6758)\n",
      "Evaluating: Batch 6645/6645: Loss: 1.8914 | Dev Acc: 31.874% (2118/6645)\n",
      "Training: Epoch 4 - Batch 1/68: Loss: 1.7472 | Train Acc: 31.000% (31/100)\n",
      "Training: Epoch 4 - Batch 2/68: Loss: 1.7366 | Train Acc: 33.500% (67/200)\n",
      "Training: Epoch 4 - Batch 3/68: Loss: 1.7701 | Train Acc: 32.000% (96/300)\n",
      "Training: Epoch 4 - Batch 4/68: Loss: 1.7502 | Train Acc: 32.500% (130/400)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch 4 - Batch 5/68: Loss: 1.7828 | Train Acc: 31.800% (159/500)\n",
      "Training: Epoch 4 - Batch 6/68: Loss: 1.7832 | Train Acc: 33.667% (202/600)\n",
      "Training: Epoch 4 - Batch 7/68: Loss: 1.7975 | Train Acc: 33.571% (235/700)\n",
      "Training: Epoch 4 - Batch 8/68: Loss: 1.8229 | Train Acc: 32.875% (263/800)\n",
      "Training: Epoch 4 - Batch 9/68: Loss: 1.8094 | Train Acc: 33.444% (301/900)\n",
      "Training: Epoch 4 - Batch 10/68: Loss: 1.7936 | Train Acc: 34.400% (344/1000)\n",
      "Training: Epoch 4 - Batch 11/68: Loss: 1.7995 | Train Acc: 34.000% (374/1100)\n",
      "Training: Epoch 4 - Batch 12/68: Loss: 1.7917 | Train Acc: 34.583% (415/1200)\n",
      "Training: Epoch 4 - Batch 13/68: Loss: 1.7870 | Train Acc: 34.462% (448/1300)\n",
      "Training: Epoch 4 - Batch 14/68: Loss: 1.7741 | Train Acc: 34.357% (481/1400)\n",
      "Training: Epoch 4 - Batch 15/68: Loss: 1.7669 | Train Acc: 34.267% (514/1500)\n",
      "Training: Epoch 4 - Batch 16/68: Loss: 1.7612 | Train Acc: 34.688% (555/1600)\n",
      "Training: Epoch 4 - Batch 17/68: Loss: 1.7654 | Train Acc: 34.529% (587/1700)\n",
      "Training: Epoch 4 - Batch 18/68: Loss: 1.7659 | Train Acc: 34.500% (621/1800)\n",
      "Training: Epoch 4 - Batch 19/68: Loss: 1.7659 | Train Acc: 34.579% (657/1900)\n",
      "Training: Epoch 4 - Batch 20/68: Loss: 1.7668 | Train Acc: 34.250% (685/2000)\n",
      "Training: Epoch 4 - Batch 21/68: Loss: 1.7653 | Train Acc: 33.952% (713/2100)\n",
      "Training: Epoch 4 - Batch 22/68: Loss: 1.7604 | Train Acc: 34.182% (752/2200)\n",
      "Training: Epoch 4 - Batch 23/68: Loss: 1.7642 | Train Acc: 33.783% (777/2300)\n",
      "Training: Epoch 4 - Batch 24/68: Loss: 1.7731 | Train Acc: 33.458% (803/2400)\n",
      "Training: Epoch 4 - Batch 25/68: Loss: 1.7693 | Train Acc: 33.600% (840/2500)\n",
      "Training: Epoch 4 - Batch 26/68: Loss: 1.7678 | Train Acc: 33.692% (876/2600)\n",
      "Training: Epoch 4 - Batch 27/68: Loss: 1.7676 | Train Acc: 33.889% (915/2700)\n",
      "Training: Epoch 4 - Batch 28/68: Loss: 1.7619 | Train Acc: 34.036% (953/2800)\n",
      "Training: Epoch 4 - Batch 29/68: Loss: 1.7609 | Train Acc: 34.103% (989/2900)\n",
      "Training: Epoch 4 - Batch 30/68: Loss: 1.7664 | Train Acc: 33.967% (1019/3000)\n",
      "Training: Epoch 4 - Batch 31/68: Loss: 1.7697 | Train Acc: 33.645% (1043/3100)\n",
      "Training: Epoch 4 - Batch 32/68: Loss: 1.7710 | Train Acc: 33.688% (1078/3200)\n",
      "Training: Epoch 4 - Batch 33/68: Loss: 1.7660 | Train Acc: 34.212% (1129/3300)\n",
      "Training: Epoch 4 - Batch 34/68: Loss: 1.7657 | Train Acc: 34.088% (1159/3400)\n",
      "Training: Epoch 4 - Batch 35/68: Loss: 1.7645 | Train Acc: 34.086% (1193/3500)\n",
      "Training: Epoch 4 - Batch 36/68: Loss: 1.7643 | Train Acc: 34.028% (1225/3600)\n",
      "Training: Epoch 4 - Batch 37/68: Loss: 1.7656 | Train Acc: 34.000% (1258/3700)\n",
      "Training: Epoch 4 - Batch 38/68: Loss: 1.7661 | Train Acc: 33.947% (1290/3800)\n",
      "Training: Epoch 4 - Batch 39/68: Loss: 1.7628 | Train Acc: 34.103% (1330/3900)\n",
      "Training: Epoch 4 - Batch 40/68: Loss: 1.7611 | Train Acc: 34.075% (1363/4000)\n",
      "Training: Epoch 4 - Batch 41/68: Loss: 1.7628 | Train Acc: 33.951% (1392/4100)\n",
      "Training: Epoch 4 - Batch 42/68: Loss: 1.7607 | Train Acc: 34.048% (1430/4200)\n",
      "Training: Epoch 4 - Batch 43/68: Loss: 1.7615 | Train Acc: 34.163% (1469/4300)\n",
      "Training: Epoch 4 - Batch 44/68: Loss: 1.7609 | Train Acc: 34.114% (1501/4400)\n",
      "Training: Epoch 4 - Batch 45/68: Loss: 1.7631 | Train Acc: 34.089% (1534/4500)\n",
      "Training: Epoch 4 - Batch 46/68: Loss: 1.7624 | Train Acc: 34.065% (1567/4600)\n",
      "Training: Epoch 4 - Batch 47/68: Loss: 1.7614 | Train Acc: 34.128% (1604/4700)\n",
      "Training: Epoch 4 - Batch 48/68: Loss: 1.7628 | Train Acc: 34.021% (1633/4800)\n",
      "Training: Epoch 4 - Batch 49/68: Loss: 1.7628 | Train Acc: 33.959% (1664/4900)\n",
      "Training: Epoch 4 - Batch 50/68: Loss: 1.7613 | Train Acc: 33.980% (1699/5000)\n",
      "Training: Epoch 4 - Batch 51/68: Loss: 1.7626 | Train Acc: 33.922% (1730/5100)\n",
      "Training: Epoch 4 - Batch 52/68: Loss: 1.7632 | Train Acc: 33.788% (1757/5200)\n",
      "Training: Epoch 4 - Batch 53/68: Loss: 1.7631 | Train Acc: 33.830% (1793/5300)\n",
      "Training: Epoch 4 - Batch 54/68: Loss: 1.7623 | Train Acc: 33.852% (1828/5400)\n",
      "Training: Epoch 4 - Batch 55/68: Loss: 1.7639 | Train Acc: 33.745% (1856/5500)\n",
      "Training: Epoch 4 - Batch 56/68: Loss: 1.7675 | Train Acc: 33.554% (1879/5600)\n",
      "Training: Epoch 4 - Batch 57/68: Loss: 1.7672 | Train Acc: 33.561% (1913/5700)\n",
      "Training: Epoch 4 - Batch 58/68: Loss: 1.7657 | Train Acc: 33.638% (1951/5800)\n",
      "Training: Epoch 4 - Batch 59/68: Loss: 1.7661 | Train Acc: 33.576% (1981/5900)\n",
      "Training: Epoch 4 - Batch 60/68: Loss: 1.7653 | Train Acc: 33.667% (2020/6000)\n",
      "Training: Epoch 4 - Batch 61/68: Loss: 1.7634 | Train Acc: 33.754% (2059/6100)\n",
      "Training: Epoch 4 - Batch 62/68: Loss: 1.7615 | Train Acc: 33.903% (2102/6200)\n",
      "Training: Epoch 4 - Batch 63/68: Loss: 1.7620 | Train Acc: 33.905% (2136/6300)\n",
      "Training: Epoch 4 - Batch 64/68: Loss: 1.7628 | Train Acc: 33.953% (2173/6400)\n",
      "Training: Epoch 4 - Batch 65/68: Loss: 1.7595 | Train Acc: 34.046% (2213/6500)\n",
      "Training: Epoch 4 - Batch 66/68: Loss: 1.7595 | Train Acc: 34.015% (2245/6600)\n",
      "Training: Epoch 4 - Batch 67/68: Loss: 1.7590 | Train Acc: 34.134% (2287/6700)\n",
      "Training: Epoch 4 - Batch 68/68: Loss: 1.7602 | Train Acc: 34.108% (2305/6758)\n",
      "Evaluating: Batch 6645/6645: Loss: 1.8753 | Dev Acc: 30.173% (2005/6645)\n",
      "Evaluating: Batch 1/69: Loss: 4.5800 | Test Acc: 0.000% (0/100)\n",
      "Evaluating: Batch 2/69: Loss: 4.2666 | Test Acc: 0.000% (0/200)\n",
      "Evaluating: Batch 3/69: Loss: 3.8956 | Test Acc: 3.000% (9/300)\n",
      "Evaluating: Batch 4/69: Loss: 3.3893 | Test Acc: 12.750% (51/400)\n",
      "Evaluating: Batch 5/69: Loss: 3.1369 | Test Acc: 16.000% (80/500)\n",
      "Evaluating: Batch 6/69: Loss: 3.0097 | Test Acc: 17.167% (103/600)\n",
      "Evaluating: Batch 7/69: Loss: 3.0098 | Test Acc: 16.714% (117/700)\n",
      "Evaluating: Batch 8/69: Loss: 3.1079 | Test Acc: 14.625% (117/800)\n",
      "Evaluating: Batch 9/69: Loss: 2.9866 | Test Acc: 14.000% (126/900)\n",
      "Evaluating: Batch 10/69: Loss: 2.8808 | Test Acc: 12.900% (129/1000)\n",
      "Evaluating: Batch 11/69: Loss: 2.7892 | Test Acc: 12.091% (133/1100)\n",
      "Evaluating: Batch 12/69: Loss: 2.7109 | Test Acc: 11.417% (137/1200)\n",
      "Evaluating: Batch 13/69: Loss: 2.6433 | Test Acc: 11.000% (143/1300)\n",
      "Evaluating: Batch 14/69: Loss: 2.5824 | Test Acc: 10.786% (151/1400)\n",
      "Evaluating: Batch 15/69: Loss: 2.5336 | Test Acc: 10.200% (153/1500)\n",
      "Evaluating: Batch 16/69: Loss: 2.4933 | Test Acc: 9.812% (157/1600)\n",
      "Evaluating: Batch 17/69: Loss: 2.4540 | Test Acc: 9.471% (161/1700)\n",
      "Evaluating: Batch 18/69: Loss: 2.4217 | Test Acc: 9.056% (163/1800)\n",
      "Evaluating: Batch 19/69: Loss: 2.4227 | Test Acc: 8.947% (170/1900)\n",
      "Evaluating: Batch 20/69: Loss: 2.4353 | Test Acc: 9.000% (180/2000)\n",
      "Evaluating: Batch 21/69: Loss: 2.4459 | Test Acc: 8.905% (187/2100)\n",
      "Evaluating: Batch 22/69: Loss: 2.4309 | Test Acc: 8.500% (187/2200)\n",
      "Evaluating: Batch 23/69: Loss: 2.4198 | Test Acc: 8.174% (188/2300)\n",
      "Evaluating: Batch 24/69: Loss: 2.4072 | Test Acc: 7.917% (190/2400)\n",
      "Evaluating: Batch 25/69: Loss: 2.3969 | Test Acc: 7.640% (191/2500)\n",
      "Evaluating: Batch 26/69: Loss: 2.3862 | Test Acc: 7.423% (193/2600)\n",
      "Evaluating: Batch 27/69: Loss: 2.3779 | Test Acc: 7.148% (193/2700)\n",
      "Evaluating: Batch 28/69: Loss: 2.3659 | Test Acc: 7.000% (196/2800)\n",
      "Evaluating: Batch 29/69: Loss: 2.3602 | Test Acc: 6.759% (196/2900)\n",
      "Evaluating: Batch 30/69: Loss: 2.3527 | Test Acc: 6.567% (197/3000)\n",
      "Evaluating: Batch 31/69: Loss: 2.3472 | Test Acc: 6.355% (197/3100)\n",
      "Evaluating: Batch 32/69: Loss: 2.3418 | Test Acc: 6.781% (217/3200)\n",
      "Evaluating: Batch 33/69: Loss: 2.3366 | Test Acc: 7.182% (237/3300)\n",
      "Evaluating: Batch 34/69: Loss: 2.3078 | Test Acc: 9.059% (308/3400)\n",
      "Evaluating: Batch 35/69: Loss: 2.2721 | Test Acc: 11.057% (387/3500)\n",
      "Evaluating: Batch 36/69: Loss: 2.2384 | Test Acc: 13.111% (472/3600)\n",
      "Evaluating: Batch 37/69: Loss: 2.2055 | Test Acc: 15.108% (559/3700)\n",
      "Evaluating: Batch 38/69: Loss: 2.1766 | Test Acc: 16.763% (637/3800)\n",
      "Evaluating: Batch 39/69: Loss: 2.1470 | Test Acc: 18.359% (716/3900)\n",
      "Evaluating: Batch 40/69: Loss: 2.1203 | Test Acc: 19.875% (795/4000)\n",
      "Evaluating: Batch 41/69: Loss: 2.0938 | Test Acc: 21.634% (887/4100)\n",
      "Evaluating: Batch 42/69: Loss: 2.0686 | Test Acc: 23.048% (968/4200)\n",
      "Evaluating: Batch 43/69: Loss: 2.0454 | Test Acc: 24.419% (1050/4300)\n",
      "Evaluating: Batch 44/69: Loss: 2.0240 | Test Acc: 25.659% (1129/4400)\n",
      "Evaluating: Batch 45/69: Loss: 2.0026 | Test Acc: 26.889% (1210/4500)\n",
      "Evaluating: Batch 46/69: Loss: 1.9820 | Test Acc: 28.217% (1298/4600)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Batch 47/69: Loss: 1.9614 | Test Acc: 29.489% (1386/4700)\n",
      "Evaluating: Batch 48/69: Loss: 1.9413 | Test Acc: 30.625% (1470/4800)\n",
      "Evaluating: Batch 49/69: Loss: 1.9232 | Test Acc: 31.653% (1551/4900)\n",
      "Evaluating: Batch 50/69: Loss: 1.9057 | Test Acc: 32.720% (1636/5000)\n",
      "Evaluating: Batch 51/69: Loss: 1.8892 | Test Acc: 33.647% (1716/5100)\n",
      "Evaluating: Batch 52/69: Loss: 1.8725 | Test Acc: 34.635% (1801/5200)\n",
      "Evaluating: Batch 53/69: Loss: 1.8572 | Test Acc: 35.566% (1885/5300)\n",
      "Evaluating: Batch 54/69: Loss: 1.8420 | Test Acc: 36.537% (1973/5400)\n",
      "Evaluating: Batch 55/69: Loss: 1.8265 | Test Acc: 37.436% (2059/5500)\n",
      "Evaluating: Batch 56/69: Loss: 1.8245 | Test Acc: 36.964% (2070/5600)\n",
      "Evaluating: Batch 57/69: Loss: 1.8219 | Test Acc: 36.544% (2083/5700)\n",
      "Evaluating: Batch 58/69: Loss: 1.8206 | Test Acc: 36.034% (2090/5800)\n",
      "Evaluating: Batch 59/69: Loss: 1.8208 | Test Acc: 35.492% (2094/5900)\n",
      "Evaluating: Batch 60/69: Loss: 1.8196 | Test Acc: 35.000% (2100/6000)\n",
      "Evaluating: Batch 61/69: Loss: 1.8194 | Test Acc: 34.492% (2104/6100)\n",
      "Evaluating: Batch 62/69: Loss: 1.8184 | Test Acc: 34.065% (2112/6200)\n",
      "Evaluating: Batch 63/69: Loss: 1.8162 | Test Acc: 33.667% (2121/6300)\n",
      "Evaluating: Batch 64/69: Loss: 1.8156 | Test Acc: 33.266% (2129/6400)\n",
      "Evaluating: Batch 65/69: Loss: 1.8143 | Test Acc: 32.877% (2137/6500)\n",
      "Evaluating: Batch 66/69: Loss: 1.8345 | Test Acc: 32.394% (2138/6600)\n",
      "Evaluating: Batch 67/69: Loss: 1.8625 | Test Acc: 31.925% (2139/6700)\n",
      "Evaluating: Batch 68/69: Loss: 1.8877 | Test Acc: 31.471% (2140/6800)\n",
      "Evaluating: Batch 69/69: Loss: 1.9181 | Test Acc: 31.374% (2140/6821)\n",
      "[1.958249238817727, 1.9055159243606783, 1.891379269631986, 1.8752619047463388]\n",
      "[28.969149736644095, 31.211437170805116, 31.873589164785553, 30.17306245297216]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "#--- hyperparameters ---\n",
    "N_EPOCHS = 4\n",
    "BATCH_SIZE_TRAIN = 100\n",
    "BATCH_SIZE_TEST = 100\n",
    "LR = 0.0003\n",
    "WEIGHT_DECAY = 0.0000\n",
    "MOMENTUM = 0.1\n",
    "\n",
    "\n",
    "#--- fixed constants ---\n",
    "NUM_CLASSES = 14\n",
    "NUM_CHANNELS = 3\n",
    "\n",
    "\n",
    "\n",
    "# --- Dataset initialization ---\n",
    "\n",
    "# We transform image files' contents to tensors\n",
    "# Plus, we can add random transformations to the training data if we like\n",
    "# Think on what kind of transformations may be meaningful for this data.\n",
    "# Eg., horizontal-flip is definitely a bad idea for sign language data.\n",
    "# You can use another transformation here if you find a better one.\n",
    "train_transform = transforms.Compose([\n",
    "                                        #transforms.RandomHorizontalFlip(),\n",
    "                                        transforms.ToTensor()])\n",
    "test_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_set = datasets.ImageFolder(TRAIN_DIR, transform=train_transform)\n",
    "dev_set   = datasets.ImageFolder(DEV_DIR,   transform=test_transform)\n",
    "test_set  = datasets.ImageFolder(TEST_DIR,  transform=test_transform)\n",
    "\n",
    "# Create Pytorch data loaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=BATCH_SIZE_TEST, shuffle=False)\n",
    "dev_loader = torch.utils.data.DataLoader(dataset=dev_set, shuffle=False)\n",
    "\n",
    "\n",
    "#--- model ---\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(NUM_CHANNELS, 20, (5,5)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2), stride = (2,2)),\n",
    "            nn.Conv2d(20, 50, (5,5)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2), stride = (2,2))\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(42050, 100),\n",
    "            nn.ReLU(),\n",
    "            # nn.Tanh(),\n",
    "            nn.Linear(100, num_classes),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "#--- set up ---\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model = CNN().to(device)\n",
    "\n",
    "# WRITE CODE HERE\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY, momentum=MOMENTUM)\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "dev_loss = math.inf\n",
    "dev_losses = []\n",
    "dev_accuracies = []\n",
    "stop_early = False\n",
    "\n",
    "#--- training ---\n",
    "for epoch in range(N_EPOCHS):\n",
    "    if stop_early:\n",
    "        break\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    total = 0\n",
    "    for batch_num, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # WRITE CODE HERE\n",
    "        pred = model(data)\n",
    "        loss = loss_function(pred, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total += len(data)\n",
    "        train_loss += loss.item()\n",
    "        train_correct += (pred.argmax(1) == target).type(torch.float).sum().item()\n",
    "\n",
    "        print('Training: Epoch %d - Batch %d/%d: Loss: %.4f | Train Acc: %.3f%% (%d/%d)' % \n",
    "              (epoch+1, batch_num+1, len(train_loader), train_loss / (batch_num + 1), \n",
    "               100. * train_correct / total, train_correct, total))\n",
    "    \n",
    "    # WRITE CODE HERE\n",
    "    # Please implement early stopping here.\n",
    "    # You can try different versions, simplest way is to calculate the dev error and\n",
    "    # compare this with the previous dev error, stopping if the error has grown.\n",
    "    cur_dev_loss = 0\n",
    "    dev_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_num, (data, target) in enumerate(dev_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # WRITE CODE HERE\n",
    "            pred = model(data)\n",
    "            loss = loss_function(pred, target)\n",
    "\n",
    "            total += len(data)\n",
    "            cur_dev_loss += loss.item()\n",
    "            dev_correct += (pred.argmax(1) == target).type(torch.float).sum().item()\n",
    "\n",
    "        current_loss = cur_dev_loss / (len(dev_loader) + 1)\n",
    "        dev_losses.append(current_loss)\n",
    "        current_accuracy = 100. * dev_correct / total\n",
    "        dev_accuracies.append(current_accuracy)\n",
    "\n",
    "        if current_loss <= dev_loss:\n",
    "            dev_loss = current_loss\n",
    "        else:\n",
    "            stop_early = True\n",
    "\n",
    "        print('Evaluating: Batch %d/%d: Loss: %.4f | Dev Acc: %.3f%% (%d/%d)' % \n",
    "            (batch_num+1, len(dev_loader), cur_dev_loss / (len(dev_loader) + 1), \n",
    "            100. * dev_correct / total, dev_correct, total))\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--- test ---\n",
    "test_loss = 0\n",
    "test_correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_num, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # WRITE CODE HERE\n",
    "        pred = model(data)\n",
    "        loss = loss_function(pred, target)\n",
    "\n",
    "        total += len(data)\n",
    "        test_loss += loss.item()\n",
    "        test_correct += (pred.argmax(1) == target).type(torch.float).sum().item()\n",
    "\n",
    "        print('Evaluating: Batch %d/%d: Loss: %.4f | Test Acc: %.3f%% (%d/%d)' % \n",
    "              (batch_num+1, len(test_loader), test_loss / (batch_num + 1), \n",
    "               100. * test_correct / total, test_correct, total))\n",
    "print(dev_losses)\n",
    "print(dev_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe67945c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
