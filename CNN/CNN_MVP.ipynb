{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e029d033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "#\n",
    "# MVP: create a version that allows us to pass and no more"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ded5e586",
   "metadata": {},
   "source": [
    "Copied from assignment 2. NEEDS WORK!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccd4ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "#--- hyperparameters ---\n",
    "N_EPOCHS = 10\n",
    "BATCH_SIZE_TRAIN = 100\n",
    "BATCH_SIZE_TEST = 100\n",
    "LR = 0.0003\n",
    "WEIGHT_DECAY = 0.0000\n",
    "MOMENTUM = 0.1\n",
    "\n",
    "\n",
    "#--- fixed constants ---\n",
    "NUM_CLASSES = 24\n",
    "DATA_DIR = '../data/sign_mnist_%s'\n",
    "\n",
    "NUM_CHANNELS = 3\n",
    "\n",
    "\n",
    "\n",
    "# --- Dataset initialization ---\n",
    "\n",
    "# We transform image files' contents to tensors\n",
    "# Plus, we can add random transformations to the training data if we like\n",
    "# Think on what kind of transformations may be meaningful for this data.\n",
    "# Eg., horizontal-flip is definitely a bad idea for sign language data.\n",
    "# You can use another transformation here if you find a better one.\n",
    "train_transform = transforms.Compose([\n",
    "                                        #transforms.RandomHorizontalFlip(),\n",
    "                                        transforms.ToTensor()])\n",
    "test_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_set = datasets.ImageFolder(DATA_DIR % 'train', transform=train_transform)\n",
    "dev_set   = datasets.ImageFolder(DATA_DIR % 'dev',   transform=test_transform)\n",
    "test_set  = datasets.ImageFolder(DATA_DIR % 'test',  transform=test_transform)\n",
    "\n",
    "\n",
    "\n",
    "# Create Pytorch data loaders\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=BATCH_SIZE_TEST, shuffle=False)\n",
    "dev_loader = torch.utils.data.DataLoader(dataset=dev_set, shuffle=False)\n",
    "\n",
    "\n",
    "#--- model ---\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super(CNN, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(NUM_CHANNELS, 20, (5,5)),\n",
    "            # nn.BatchNorm2d(20), \n",
    "            nn.ReLU(),\n",
    "            # nn.Tanh(),\n",
    "            nn.MaxPool2d((2,2), stride = (2,2)),\n",
    "            nn.Conv2d(20, 50, (5,5)),\n",
    "            # nn.BatchNorm2d(50),\n",
    "            nn.ReLU(),\n",
    "            # nn.Tanh(),\n",
    "            nn.MaxPool2d((2,2), stride = (2,2))\n",
    "        )\n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(4*4*50, 500),\n",
    "            nn.ReLU(),\n",
    "            # nn.Tanh(),\n",
    "            nn.Linear(500, num_classes),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.lin(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "#--- set up ---\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model = CNN().to(device)\n",
    "\n",
    "# WRITE CODE HERE\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY, momentum=MOMENTUM)\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "dev_loss = math.inf\n",
    "dev_losses = []\n",
    "dev_accuracies = []\n",
    "stop_early = False\n",
    "\n",
    "#--- training ---\n",
    "for epoch in range(N_EPOCHS):\n",
    "    if stop_early:\n",
    "        break\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    total = 0\n",
    "    for batch_num, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # WRITE CODE HERE\n",
    "        pred = model(data)\n",
    "        loss = loss_function(pred, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total += len(data)\n",
    "        train_loss += loss.item()\n",
    "        train_correct += (pred.argmax(1) == target).type(torch.float).sum().item()\n",
    "\n",
    "        print('Training: Epoch %d - Batch %d/%d: Loss: %.4f | Train Acc: %.3f%% (%d/%d)' % \n",
    "              (epoch+1, batch_num+1, len(train_loader), train_loss / (batch_num + 1), \n",
    "               100. * train_correct / total, train_correct, total))\n",
    "    \n",
    "    # WRITE CODE HERE\n",
    "    # Please implement early stopping here.\n",
    "    # You can try different versions, simplest way is to calculate the dev error and\n",
    "    # compare this with the previous dev error, stopping if the error has grown.\n",
    "    cur_dev_loss = 0\n",
    "    dev_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_num, (data, target) in enumerate(dev_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # WRITE CODE HERE\n",
    "            pred = model(data)\n",
    "            loss = loss_function(pred, target)\n",
    "\n",
    "            total += len(data)\n",
    "            cur_dev_loss += loss.item()\n",
    "            dev_correct += (pred.argmax(1) == target).type(torch.float).sum().item()\n",
    "\n",
    "        current_loss = cur_dev_loss / (len(dev_loader) + 1)\n",
    "        dev_losses.append(current_loss)\n",
    "        current_accuracy = 100. * dev_correct / total\n",
    "        dev_accuracies.append(current_accuracy)\n",
    "\n",
    "        if current_loss <= dev_loss:\n",
    "            dev_loss = current_loss\n",
    "        else:\n",
    "            stop_early = True\n",
    "\n",
    "        print('Evaluating: Batch %d/%d: Loss: %.4f | Dev Acc: %.3f%% (%d/%d)' % \n",
    "            (batch_num+1, len(dev_loader), cur_dev_loss / (len(dev_loader) + 1), \n",
    "            100. * dev_correct / total, dev_correct, total))\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--- test ---\n",
    "test_loss = 0\n",
    "test_correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_num, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # WRITE CODE HERE\n",
    "        pred = model(data)\n",
    "        loss = loss_function(pred, target)\n",
    "\n",
    "        total += len(data)\n",
    "        test_loss += loss.item()\n",
    "        test_correct += (pred.argmax(1) == target).type(torch.float).sum().item()\n",
    "\n",
    "        print('Evaluating: Batch %d/%d: Loss: %.4f | Test Acc: %.3f%% (%d/%d)' % \n",
    "              (batch_num+1, len(test_loader), test_loss / (batch_num + 1), \n",
    "               100. * test_correct / total, test_correct, total))\n",
    "print(dev_losses)\n",
    "print(dev_accuracies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
